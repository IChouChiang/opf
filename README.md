# Optimal Power Flow (OPF) Educational Project

Power systems optimization and machine learning study using Python, Pyomo, PYPOWER, Gurobi, and PyTorch.

## ğŸ¯ Project Overview

Educational assignments progressing from DC Optimal Power Flow (Week 2) through ML-based prediction (Week 3) to AC Optimal Power Flow (Week 4).

**Key Technologies:** Pyomo, PYPOWER/MATPOWER, Gurobi, PyTorch, NumPy

**Environment:** `opf311` (Anaconda)  
**Current Phase:** Week 7-8 - Node-Wise Architecture & Generalization Testing

---

## ğŸš€ Recent Updates (Dec 2025)

### Node-Wise GCNN Architecture
- **Goal:** Achieve Inductive Generalization (train on one topology, test on others).
- **Method:** Removed the "Flattening" layer found in the original paper. Implemented a **Node-Wise Readout** where the same MLP is applied to every node independently.
- **Result:**
  - **Parameter Efficiency:** Reduced parameters from ~46k to **5,668** (~90% reduction).
  - **Seen Data:** Excellent performance (VG RÂ² > 0.999).
  - **Unseen Data:** Successfully predicted Voltage physics (VG RÂ² 0.65) but failed on Active Power (PG) due to the global nature of cost optimization.

### Configuration System
- **Legacy:** JSON config files in `gcnn_opf_01/configs/` with `model_type`: `"flattened"` (original) or `"nodewise"` (new).
- **Hydra Integration:** Unified configuration system in `configs/` directory:
  - `config.yaml` - Main configuration file
  - `model/` - Model-specific configurations (DNN, GCNN)
  - `data/` - Dataset configurations (case6, case39)
  - Command-line overrides: `python scripts/train.py model=dnn data=case39 train.max_epochs=50`
  - Test: `python tests/test_hydra_train.py` - Verifies Hydra configuration system integration

### Unified Model Refactoring (Dec 2025)
- **New Package:** `src/deep_opf/` - Unified deep learning framework for OPF
- **Models:** 
  - `AdmittanceDNN`: Fully connected network for flat feature input (legacy Model 03)
  - `GCNN`: Physics-guided graph convolutional network (legacy Model 01)
- **Data Loading:** 
  - `OPFDataset`: Unified PyTorch Dataset supporting 'flat' and 'graph' feature types
  - `OPFDataModule`: PyTorch Lightning DataModule for streamlined training
- **Configuration:** 
  - `configs/` - Hydra configuration system for model/data/training parameters
  - `scripts/train.py` - Unified training script with Hydra integration
- **Verification:** 
  - `tests/verify_models.py` - Comprehensive model validation script
  - `tests/test_hydra_train.py` - Hydra configuration system integration test

---

## ğŸ“ Project Structure

```
opf/
â”œâ”€â”€ weekly_assignments/ # Progressive learning modules
â”‚   â”œâ”€â”€ Week2/          # DC-OPF: linear formulation, case9
â”‚   â”œâ”€â”€ Week3/          # ML prediction: DCOPF â†’ MLP, case118
â”‚   â”œâ”€â”€ Week5/          # GCNN project documentation (Chinese)
â”‚   â”œâ”€â”€ Week6/          # Advanced topics
â”‚   â””â”€â”€ Week7to8/       # Physics-Informed ML (Model 01 vs Model 03)
â”œâ”€â”€ gcnn_opf_01/        # Physics-guided GCNN for OPF (case6ww)
â”‚   â”œâ”€â”€ data/           # 12k samples (10k train, 2k test) [git-ignored]
â”‚   â”œâ”€â”€ results/        # Training results & tuning [git-ignored]
â”‚   â”œâ”€â”€ docs/           # Design documentation
â”‚   â”‚   â”œâ”€â”€ gcnn_opf_01.md           # Main project notes & status
â”‚   â”‚   â”œâ”€â”€ formulas_model_01.md     # Mathematical formulas
â”‚   â”‚   â””â”€â”€ *.md                     # Other guides
â”‚   â”œâ”€â”€ model_01.py                  # 2-head GCNN architecture
â”‚   â”œâ”€â”€ loss_model_01.py             # Physics-informed loss functions
â”‚   â”œâ”€â”€ feature_construction_model_01.py  # Model-informed features (Sec III-C)
â”‚   â”œâ”€â”€ sample_config_model_01.py    # case6ww config & operators
â”‚   â”œâ”€â”€ sample_generator_model_01.py # RES scenario generator
â”‚   â”œâ”€â”€ config_model_01.py           # Dataclass configs
â”‚   â”œâ”€â”€ train.py                     # Training pipeline
â”‚   â”œâ”€â”€ evaluate.py                  # Model evaluation
â”‚   â””â”€â”€ tune_batch_size.py           # Hyperparameter tuning (with caching)
â”œâ”€â”€ src/                # Reusable modules
â”‚   â”œâ”€â”€ deep_opf/        # Unified deep learning framework
â”‚   â”‚   â”œâ”€â”€ data/        # Dataset and DataModule classes
â”‚   â”‚   â”‚   â”œâ”€â”€ dataset.py      # OPFDataset (flat/graph features)
â”‚   â”‚   â”‚   â”œâ”€â”€ datamodule.py   # OPFDataModule (Lightning)
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ models/      # Neural network architectures
â”‚   â”‚   â”‚   â”œâ”€â”€ dnn.py          # AdmittanceDNN (MLP)
â”‚   â”‚   â”‚   â”œâ”€â”€ gcnn.py         # GCNN (graph convolution)
â”‚   â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”œâ”€â”€ ac_opf_create.py       # Pyomo AbstractModel (Cartesian voltages)
â”‚   â””â”€â”€ helpers_ac_opf.py      # AC-OPF helpers (data prep, init, solve)
â”œâ”€â”€ tests/              # Test harnesses and baselines
â”‚   â”œâ”€â”€ verify_models.py       # DNN/GCNN model verification
â”‚   â”œâ”€â”€ test_case39.py         # IEEE 39-bus AC-OPF
â”‚   â”œâ”€â”€ test_case57.py         # IEEE 57-bus AC-OPF
â”‚   â”œâ”€â”€ test_feature_construction.py  # Feature construction validation
â”‚   â”œâ”€â”€ test_sample_generator.py      # Scenario generator + AC-OPF
â”‚   â”œâ”€â”€ test_topology_outages.py      # N-1 contingency verification
â”‚   â”œâ”€â”€ case39_baseline.py     # PYPOWER reference (39-bus)
â”‚   â””â”€â”€ case57_baseline.py     # PYPOWER reference (57-bus)
â”œâ”€â”€ outputs/            # Generated files (git-ignored)
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ copilot-instructions.md
â”œâ”€â”€ pyrightconfig.json
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### Environment setup
```bash
conda activate opf311
```

### Week 4 AC-OPF (Current)
Run the AC-OPF test harnesses:
```bash
cd tests
python test_case39.py   # IEEE 39-bus
python test_case57.py   # IEEE 57-bus
```

Baseline comparison (PYPOWER):
```bash
python case39_baseline.py
python case57_baseline.py
```

---

## ğŸ“Š Week 4 Highlights (AC-OPF)

### Features
- **Cartesian voltage formulation:** Variables `e[i]` (real) and `f[i]` (imag) instead of polar Vm/Va
- **Fixed quadratic objective:** Minimize Î£(aÂ·PGÂ² + bÂ·PG + c) with cost coefficients scaled for p.u. variables
- **Nonlinear power balance:** Bilinear constraints using admittance matrix G, B from PYPOWER's `makeYbus`
- **Voltage magnitude limits:** (Vmin)Â² ï¿½?eÂ² + fÂ² ï¿½?(Vmax)Â²
- **Gurobi NonConvex solver:** MIQCP with spatial branching, half CPU cores, 3-minute time limit, 3% MIP gap

### Shared helpers (src/helpers_ac_opf.py)
- `prepare_ac_opf_data(ppc)`: ext2int, Ybusâ†’G/B, per-unit scaling, cost params
- `initialize_voltage_from_flatstart(instance, ppc_int)`: set e/f from Vm/Va
- `solve_ac_opf(ppc, verbose=True, time_limit=180, mip_gap=0.03, threads=None)`: build, init (PG/QG, slack fix), solve

### Results (tests/)
- **IEEE 39-bus:** 41872.30 $/hr (vs PYPOWER 41864.18, ~0.02% gap), ~2s solve
- **IEEE 57-bus:** 41770.00 $/hr (~1% gap), ~130s solve

### Technical Notes
- Cost scaling: For PG in per-unit, use `a = c2Â·baseMVAÂ²`, `b = c1Â·baseMVA`, `c = c0` to preserve $/hr units
- Slack bus voltage fixed to eliminate rotational symmetry
- Generator PG/QG initialized from case data for warm start
- External 1-based bus/gen numbering in output (matches PYPOWER convention)

---

## ğŸ§© Dependencies

- `pyomo` ï¿½?optimization modeling
- `pypower` ï¿½?power flow cases and reference solver
- `gurobipy` ï¿½?nonconvex quadratic solver
- `torch` ï¿½?neural network training (Week 3)
- `numpy`, `matplotlib`

See `.github/copilot-instructions.md` for detailed architecture patterns and workflow.

---

## ğŸ“ Development Notes

- **Type checking:** `pyrightconfig.json` configured; use `# pyright: reportAttributeAccessIssue=false` in Pyomo files
- **Units:** Always convert MW/MVAr to p.u. via `baseMVA` (typically 100.0)
- **MATPOWER compatibility:** Bus/gen/branch matrices follow MATPOWER column indexing (0-based in NumPy)

---

## ğŸ§  GCNN OPF Subproject (gcnn_opf_01/)

### Overview
Physics-guided Graph Convolutional Neural Network for optimal power flow prediction on **case6ww** (6-bus Wood & Wollenberg system).

### Architecture
- **Model:** 2Ã—GraphConv ï¿½?shared FC ï¿½?two heads
  - `gen_head`: [N_GEN=3, 2] ï¿½?(PG, VG)
  - `v_head`: [N_BUS=6, 2] ï¿½?(e, f) for physics validation
- **Feature construction:** k=8 iterations of model-informed voltage estimation (Section III-C)
  - Iterative PG/QG computation with generator clamping (Eqs. 23-24)
  - Voltage updates via power flow equations (Eqs. 16-17, 19-22)
  - Voltage magnitude normalization (Eq. 25)
- **Loss:** L_supervised + ÎºÂ·L_Î”,PG (correlative physics-informed loss)
  - Supervised: MSE on (PG, VG) predictions
  - Physics: MSE on power balance residuals using predicted voltages

### Key Files
- `feature_construction_model_01.py`: Implements iterative voltage estimation
- `loss_model_01.py`: Physics-informed loss functions
- `model_01.py`: GCNN architecture with GraphConv layers
- `sample_config_model_01.py`: case6ww operators (G, B matrices)
- `sample_generator_model_01.py`: RES scenario generator (wind/PV)

### Testing
```bash
# Feature construction test
python tests/test_feature_construction.py  # ï¿½?Validated [6,8] features, normalized voltages

# Scenario generation + AC-OPF
python tests/test_sample_generator.py      # ï¿½?3 scenarios, 30% RES, all optimal

# Topology verification
python tests/test_topology_outages.py      # ï¿½?N-1 contingencies verified

# Hydra configuration system integration test
python tests/test_hydra_train.py           # ï¿½?Verifies Hydra configuration for DNN and GCNN models
```

### Status (Completed 2025-11-25)
- âœ… Model architecture (2-head GCNN)
- âœ… Feature construction (k=8 iterations)
- âœ… Physics-informed loss functions
- âœ… Scenario generator (Gaussian load + Weibull wind + Beta PV)
- âœ… AC-OPF integration (using `src/helpers_ac_opf.py`)
- âœ… Dataset generation (12k samples, 96% success rate)
- âœ… **Hyperparameter tuning** (batch size optimization, 16 experiments)
- âœ… Training pipeline (35 epochs, early stopping, batch_size=6)
- âœ… Model evaluation (RÂ²=98.21% for PG, RÂ²=99.99% for VG)
- âœ… Probabilistic accuracy metrics (P_PG=38.45%, P_VG=14.80%)

---

## ï¿½?Completed Milestones

- [x] Week 2: DC-OPF with linear constraints, PTDF analysis
- [x] Week 3: ML-based OPF prediction (MLP: P_D ï¿½?P_G), 10k samples
- [x] Week 4: AC-OPF Cartesian formulation, Gurobi nonconvex solve, PYPOWER baseline validation
- [x] Week 5: GCNN-OPF complete pipeline
  - [x] Model architecture (2-head GCNN with physics-informed layers)
  - [x] Feature construction (k=8 iterations)
  - [x] Dataset generation (12k samples, 5 topologies, 50.7% RES penetration)
  - [x] **Hyperparameter optimization** (batch size tuning: 16 experiments, optimal=6)
  - [x] Training (35 epochs, physics-informed loss, early stopping)
  - [x] Evaluation (RÂ²=98.21% for power, RÂ²=99.99% for voltage)
  - [x] Probabilistic accuracy metrics (P_PG=38.45%, P_VG=14.80%)
  - [x] Chinese documentation (Week5/Week5.md)

---

## ğŸ“š References

- MATPOWER documentation: https://matpower.org
- Pyomo: https://www.pyomo.org
- Gurobi NonConvex QCQP: https://www.gurobi.com/documentation/

------

## ğŸš€ Week 5 Highlights (GCNN-OPF)

### Training Results (Optimized with Batch Size Tuning)
- **Model:** 1000 neurons (FC layer), Two-Stage Training
- **Optimal Batch Size:** 24 (found via tuning for larger model)
- **Training:** 
  - Phase 1 (Supervised): 25 epochs
  - Phase 2 (Physics-Informed): 24 epochs
- **Physics loss weight (Îº):** 1.0 (in Phase 2)

### Test Set Performance (2,000 samples)
- **Generator Power (PG):**
  - **Probabilistic Accuracy (Error < 1 MW): 98.42%**
  - RÂ² = 0.9844
  - RMSE = 0.0069 p.u. (0.69 MW)
- **Generator Voltage (VG):**
  - **Probabilistic Accuracy (Error < 0.001 p.u.): 100.00%**
  - MAE = 0.0538 p.u. â‰ˆ 5.4 MW
  - MAPE = 26.32%
  - **P_PG = 38.45%** (errors < 1 MW threshold)

- **Generator Voltage (VG):**
  - RÂ² = 0.9999 (99.99% variance explained)
  - RMSE = 0.0086 p.u. â‰ˆ 0.86%
  - MAE = 0.0059 p.u. â‰ˆ 0.59%
  - MAPE = 0.56%
  - **P_VG = 14.80%** (errors < 0.001 p.u. threshold)

### Batch Size Tuning Summary
Conducted comprehensive 3-stage tuning (16 experiments total):
- **Optimal:** Batch size 6 (val_loss = 0.1460)
- **Key finding:** Small batches (2-8) significantly outperform large batches (256-1024) by 2.6-2.8x
- **Trade-off:** Large batches train 2-3x faster but sacrifice accuracy
- **Insight:** Physics-constrained GCNN benefits from frequent gradient updates with small batches

See `gcnn_opf_01/docs/gcnn_opf_01.md` for complete tuning results table.

### Dataset Details
- **System:** case6ww (6 buses, 3 generators)
- **Topologies:** 5 configurations (base + 4 N-1 contingencies)
- **RES Integration:** Wind (Weibull) at bus 5, PV (Beta) at buses 4 & 6
- **Target Penetration:** 50.7%
- **Training samples:** 10,000 (96.2% success rate)
- **Test samples:** 2,000 (95.7% success rate)

### Documentation
- Full Chinese documentation available in `Week5/Week5.md`
- Includes model architecture, sample generation, and training results

---

## ğŸ“š Additional Documentation

- **Week5/Week5.md** - Comprehensive Chinese documentation of GCNN-OPF project
- **.github/copilot-instructions.md** - Development patterns and architecture guide
- **MAINTENANCE.md** - Change log and implementation notes
- **gcnn_opf_01/*.md** - Design documents, formulas, and guides

---

## ğŸ“ References

- MATPOWER: https://matpower.org
- Pyomo: https://www.pyomo.org
- Gurobi: https://www.gurobi.com/documentation/
- Paper: "A Physics-Guided Graph Convolution Neural Network for Optimal Power Flow" (Gao et al.)
